{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import cleaner as dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83796, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_clean = pd.read_csv('../clean_dataset.csv')\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words and sequences that cab be used to filter our ads using scarcity\n",
    "negative_emotion_words = ['suffering', 'suffered','nightmare','forced','abuse','abused','burden','getting worse','horror','violence',\n",
    "                    'catastrophe','affraid','cruelty','deadly','terror', 'danger', 'dangerous',  'regret', 'regretted', 'rejection', \n",
    "                    'reject','rejected', 'risk', 'risked', 'sabotage', 'sabotaged', 'sabotaging', 'scary', 'scared','suffer', 'threat',\n",
    "                    'threatened', 'threatening', 'tragic', 'tragically', 'tragical', 'trapped' , 'trap','fail', 'fails', 'failed', 'failing', \n",
    "                    'victim', 'fooled','fooling', 'helpless', 'hurting', 'hurt', 'scared','mistake' , 'mistaken', 'neglect','neglected', 'neglecting', \n",
    "                    'pitfalls', 'powerless', 'vulnerable', 'waste', 'wasted','wasting', 'worry', 'worried', 'worrying', 'disappointed', 'dissapoint', \n",
    "                    'dissapointing','frustrated', 'frsutrating', 'greedy',  'greed','irritated',  'maddening', 'mad', 'ruthless',  'misleading', \n",
    "                    'frustration','infuriating','infuriated', 'pointless', 'reclaim', 'reclaimed', 'sick', 'tired', 'tiring', 'swindled', 'swindle',\n",
    "                    'trigger', 'triggered', 'unacceptable', 'violate', 'violated', 'uncomfortable', 'victims', 'risking', 'critcial', 'critically', \n",
    "                    'concerning', 'concerned', 'problamatic', 'drastic', 'drastiaclly', 'hardship', 'hardships', 'rigging', 'rape', 'raped', \n",
    "                    'assault', 'assaulted', 'assaulting', 'insult', 'insulting', 'murder', 'weapon',  'rigged', 'desperate', 'desperated', \n",
    "                    'danger', 'regret',  'risk', 'sabotage', 'scary','threat','tragic', 'victim',  'helpless', 'hurting','vulnerable', \n",
    "                    'frustrated', 'greedy','irritated',  'violate' ]\n",
    "                    \n",
    "# words and sequences that cab be used to filter our ads using scarcity\n",
    "positive_emotion_words = ['delighted', 'ecstatic', 'freedom', 'relaxed', 'assured', 'happy', 'healthy',\n",
    "                 'fulfilled', 'genuine', 'authentic', 'secure', 'stable', 'honest', 'truthful','supportive', 'excellent',\n",
    "                'blissful', 'joyous', 'delighted', 'overjoyed', 'gleeful', 'thankful', 'festive', 'ecstatic', 'satisfied', \n",
    "                'cheerful','sunny', 'elated', 'jubilant', 'jovial', 'fun-loving', 'lighthearted', 'glorious', 'innocent', \n",
    "                'child-like', 'gratified', 'euphoric', 'playful', 'courageous', 'energetic', 'liberated', 'optimistic', \n",
    "                'animated', 'spirited','thrilled', 'wonderful', 'funny', 'exhilarated','youthful', \n",
    "                'tickled', 'creative', 'constructive', 'comfortable', 'pleased', 'encouraged', 'surprised', \n",
    "                'content', 'serene', 'bright', 'blessed', 'vibrant', 'glowing','relaxing','pleasing',\n",
    "                'fulfilling','good quality','compassion','glad','appreciative', 'joyful', 'pleased', \n",
    "                'confident', 'cheery', 'polite', 'chilly','glory','brave', 'heroic', 'enthusiastic', \n",
    "                'passionate','optimistic','delighted', 'excited', 'amazing','amusing','innovative', 'artistic',\n",
    "                'authentic' 'supportive', 'excellent','blissful', 'joyous', 'delighted', 'overjoyed', 'gleeful', 'thankful']\n",
    "\n",
    "def contains_word(s, w):\n",
    "    if w in s.lower() :\n",
    "      return True\n",
    "    else: \n",
    "      return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Sentimet analysis model \n",
    "df = pd.read_csv('../sentiment_dataset.csv')\n",
    "print(df.shape)\n",
    "\n",
    "# clean data \n",
    "df['review'] = df['review'].apply(lambda x: x.lower())\n",
    "df['review'] = df['review'].apply(dc.remove_stopwords)\n",
    "df['review'] = df['review'].str.replace('[^\\w\\s]','', regex=True)\n",
    "df['review'] = df['review'].str.replace('\\s+', ' ',  regex=True)\n",
    "df['review'] = df['review'].apply(dc.denoise_text)\n",
    "df['review'] = df['review'].apply(dc.remove_special_characters)\n",
    "df['review'] = df['review'].apply(dc.simple_stemmer)\n",
    "\n",
    "# split training and testing dataset \n",
    "#train dataset\n",
    "norm_train=df.review[:40000]\n",
    "train_sentiments=df.sentiment[:40000]\n",
    "\n",
    "#test dataset\n",
    "norm_test=df.review[40000:]\n",
    "test_sentiments=df.sentiment[40000:]\n",
    "\n",
    "# vectorize training and testing \n",
    "#Count vectorizer for bag of words\n",
    "cv = CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "cv_train_reviews = cv.fit_transform(norm_train)\n",
    "cv_test_reviews=cv.transform(norm_test)\n",
    "\n",
    "\n",
    "# train naive bayes model for sentiment detection\n",
    "mnb=MultinomialNB()\n",
    "mnb_bow=mnb.fit(cv_train_reviews,train_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the trained model to predict sentiments in the negative emotional persuasion ads \n",
    "negative_emotion_true , negative_emotion_true_new =  set() , set() \n",
    "df_nemotion = pd.DataFrame() \n",
    "\n",
    "for sentence in df_clean['ads']:                                   \n",
    "  for word in  negative_emotion_words:                                        \n",
    "      if contains_word(sentence, word):\n",
    "        negative_emotion_true.add(sentence)\n",
    "\n",
    "# take ones that do not contain positive words\n",
    "for sentence in negative_emotion_true:                                   \n",
    "  for word in  positive_emotion_words:                                        \n",
    "      if contains_word(sentence, word):\n",
    "        negative_emotion_true_new.add(sentence)\n",
    "negative_emotion_true = negative_emotion_true_new\n",
    "\n",
    "df_negative_emotion_true = pd.DataFrame(list(negative_emotion_true), columns= ['ads'])\n",
    "df_negative_emotion_true['stemmed'] = df_negative_emotion_true['ads'].apply(dc.simple_stemmer) # stemming the text \n",
    "df_negative_emotion_true['sentiment'] = 0 # negative sentiment \n",
    "print(df_negative_emotion_true.shape)\n",
    "\n",
    "# make predictions\n",
    "norm_data = cv.transform(df_negative_emotion_true['stemmed'])\n",
    "df_negative_emotion_true['predicted']= mnb.predict(norm_data)\n",
    "df_nemotion = df_negative_emotion_true.loc[df_negative_emotion_true['predicted'] == 'negative']\n",
    "df_nemotion.drop(columns  = ['predicted', 'stemmed', 'sentiment' ], inplace = True)\n",
    "df_nemotion['emotion'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1014, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nemotion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the trained model to predict sentiments in the positive emotional persuasion ads \n",
    "positive_emotion_true , positive_emotion_true_new =  set() , set() \n",
    "df_pemotion = pd.DataFrame() \n",
    "\n",
    "for sentence in df_clean['ads']:                                   \n",
    "  for word in  positive_emotion_words:                                        \n",
    "      if contains_word(sentence, word):\n",
    "        positive_emotion_true.add(sentence)\n",
    "\n",
    "# take ones that do not contain positive words\n",
    "for sentence in positive_emotion_true:                                   \n",
    "  for word in  positive_emotion_words:                                        \n",
    "      if contains_word(sentence, word):\n",
    "        positive_emotion_true_new.add(sentence)\n",
    "positive_emotion_true = positive_emotion_true_new\n",
    "\n",
    "df_positive_emotion_true = pd.DataFrame(list(positive_emotion_true), columns= ['ads'])\n",
    "df_positive_emotion_true['stemmed'] = df_positive_emotion_true['ads'].apply(dc.simple_stemmer) # stemming the text \n",
    "df_positive_emotion_true['sentiment'] = 0 # positive sentiment \n",
    "print(df_positive_emotion_true.shape)\n",
    "\n",
    "# make predictions\n",
    "norm_data = cv.transform(df_positive_emotion_true['stemmed'])\n",
    "df_positive_emotion_true['predicted']= mnb.predict(norm_data)\n",
    "df_pemotion = df_positive_emotion_true.loc[df_positive_emotion_true['predicted'] == 'positive']\n",
    "df_pemotion.drop(columns  = ['predicted', 'stemmed', 'sentiment' ], inplace = True)\n",
    "df_pemotion['emotion'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3079, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pemotion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2214, 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emotional_appeal = pd.concat([df_nemotion, df_pemotion.sample(n = 1200)]).reset_index(drop = True)\n",
    "df_emotional_appeal.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8f14f5a7c49a331ac7a55934b43ce13bd28be1333db14e2d71768ad3378996c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
