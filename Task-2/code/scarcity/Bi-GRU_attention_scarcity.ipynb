{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 8218,
     "status": "ok",
     "timestamp": 1654159174493,
     "user": {
      "displayName": "NARDJES AMIEUR",
      "userId": "13751943947351102071"
     },
     "user_tz": -120
    },
    "id": "hLtVT3MuP132"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import scikitplot as skplt\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 1372,
     "status": "ok",
     "timestamp": 1654159177762,
     "user": {
      "displayName": "NARDJES AMIEUR",
      "userId": "13751943947351102071"
     },
     "user_tz": -120
    },
    "id": "Y7Wxle6DENLM",
    "outputId": "83c69518-6301-4aad-ea51-75b375434601"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ad</th>\n",
       "      <th>scarcity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it’s almost over the shortcut law that lets yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>on sale this week sequel to nyt bestseller pro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>refugees are especially vulnerable to the dang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we are motivating reminding and supporting vot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the future of decentralized nomadic coliving i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17995</th>\n",
       "      <td>parents take part in the study of the impact o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17996</th>\n",
       "      <td>lincoln news lincoln mayor orders greater rest...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17997</th>\n",
       "      <td>joe biden beat the nra twice he got assault we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17998</th>\n",
       "      <td>reid nearly lost his eye to cancer thanks to t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17999</th>\n",
       "      <td>new video\\nif we want real solutions we need n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      ad  scarcity\n",
       "0      it’s almost over the shortcut law that lets yo...         1\n",
       "1      on sale this week sequel to nyt bestseller pro...         1\n",
       "2      refugees are especially vulnerable to the dang...         1\n",
       "3      we are motivating reminding and supporting vot...         1\n",
       "4      the future of decentralized nomadic coliving i...         1\n",
       "...                                                  ...       ...\n",
       "17995  parents take part in the study of the impact o...         0\n",
       "17996  lincoln news lincoln mayor orders greater rest...         0\n",
       "17997  joe biden beat the nra twice he got assault we...         0\n",
       "17998  reid nearly lost his eye to cancer thanks to t...         0\n",
       "17999  new video\\nif we want real solutions we need n...         0\n",
       "\n",
       "[18000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import scarcity labeled ads\n",
    "df = pd.read_csv('data/scarcity_dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1654159180715,
     "user": {
      "displayName": "NARDJES AMIEUR",
      "userId": "13751943947351102071"
     },
     "user_tz": -120
    },
    "id": "ilUI__p5P-tR"
   },
   "outputs": [],
   "source": [
    "norm_train, norm_test,  y_train, y_test = train_test_split(df['ad'], df['scarcity'], test_size =0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4194,
     "status": "ok",
     "timestamp": 1654159186309,
     "user": {
      "displayName": "NARDJES AMIEUR",
      "userId": "13751943947351102071"
     },
     "user_tz": -120
    },
    "id": "3GzQqy98d9xx",
    "outputId": "92c9ddf7-1757-485f-e3cf-19b65d199f5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('httpbitlyyekel', 39498), ('<PAD>', 0), 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data tokenization \n",
    "t = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n",
    "\n",
    "# fit the tokenizer on the training documents\n",
    "t.fit_on_texts(norm_train)\n",
    "t.word_index['<PAD>'] = 0\n",
    "max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), t.word_index['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2427,
     "status": "ok",
     "timestamp": 1654159189679,
     "user": {
      "displayName": "NARDJES AMIEUR",
      "userId": "13751943947351102071"
     },
     "user_tz": -120
    },
    "id": "6MyoVSt1fVkZ"
   },
   "outputs": [],
   "source": [
    "# create tokenized data sequences to feed to the neural network \n",
    "train_sequences = t.texts_to_sequences(norm_train)\n",
    "test_sequences = t.texts_to_sequences(norm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1654159190018,
     "user": {
      "displayName": "NARDJES AMIEUR",
      "userId": "13751943947351102071"
     },
     "user_tz": -120
    },
    "id": "khWmrmG0g0Cg",
    "outputId": "7a1922f0-ede6-4554-f3be-5ac815ed3a71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14400, 1000), (3600, 1000))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "# pad documents to equal length with 0 \n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1654159190018,
     "user": {
      "displayName": "NARDJES AMIEUR",
      "userId": "13751943947351102071"
     },
     "user_tz": -120
    },
    "id": "Z5dHWRZPi8YE"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "            Attention Layer sourced from : https://github.com/lzfelix/keras_attention/blob/master/attention/layers.py\n",
    "\n",
    "            Implementation based in the work of Yang et al. \"Hierarchical Attention Networks for Document Classification\". \n",
    "            This implementation also allows changing the common tanh activation function used on the attention layer, as Chen\n",
    "            et al. \"A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task\"\n",
    "            point that removing this component can be beneficial to the model. Supports\n",
    "            masking.\n",
    "    \n",
    "            The mathematical formulation of the model is as follows:\n",
    "              ```\n",
    "              u = f(W * h + b),\n",
    "              a_i = softmax(u_i^T * u_s),\n",
    "              v_i = \\sigma_i a_i * h_i.\n",
    "              ```\n",
    "    \n",
    "            # Arguments\n",
    "                  activation: The activation function f used by the layer (see\n",
    "                      [activations](../activations.md)). By default tanh is used, another common\n",
    "                      option is \"linear\".\n",
    "                  use_bias: Boolean, whether the layer uses a bias vector.\n",
    "                  initializer: Initializer for the `kernel` and `context` matrices\n",
    "                      (see [initializers](../initializers.md)).\n",
    "                  return_attention: If True, instead of returning the sequence descriptor, this\n",
    "                      layer will return the computed attention coefficients for each of the\n",
    "                      sequence timesteps. See Output section for details.\n",
    "                  W_regularizer: Regularizer function applied to the `kernel` weights matrix\n",
    "                      (see [regularizer](../regularizers.md)).\n",
    "                  u_regularizer: Regularizer function applied to the `context` weights matrix\n",
    "                      (see [regularizer](../regularizers.md)).\n",
    "                  b_regularizer: Regularizer function applied to the bias vector\n",
    "                      (see [regularizer](../regularizers.md)).\n",
    "                  W_constraint: Constraint function applied to the `kernel` weights matrix\n",
    "                      (see [constraints](../constraints.md)).\n",
    "                  u_constraint: Constraint function applied to the `contextl` weights matrix\n",
    "                      (see [constraints](../constraints.md)).\n",
    "                  b_constraint: Constraint function applied to the bias vector\n",
    "                      (see [constraints](../constraints.md)).\n",
    "          # Input shape\n",
    "                  nD tensor with shape: `(batch_size, ..., timesteps, input_dim)`.\n",
    "                  The most common situation would be a 3D input with shape\n",
    "                  `(batch_size, timesteps, input_dim)`.\n",
    "          # Outuput shape\n",
    "                  The sequence descriptor with shape `(batch_size, ..., timestamps)`. If\n",
    "                  `return_attention` is True, this layer will return the `alpha_i` weights\n",
    "                  for each timestep, and consequently its output shape will be different, namely:\n",
    "                  `(batch_size, ..., timesteps)`. \n",
    "        \"\"\"\n",
    "        \n",
    "        self.supports_masking = True\n",
    "        self.init = tf.keras.initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = tf.keras.regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = tf.keras.regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = tf.keras.constraints.get(W_constraint)\n",
    "        self.b_constraint = tf.keras.constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight( shape= (input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape = (input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "        \n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        # TF backend doesn't support it\n",
    "        # eij = K.dot(x, self.W) \n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), \n",
    "                              K.reshape(self.W, (features_dim, 1))),\n",
    "                        (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'step_dim': self.step_dim}\n",
    "        base_config = super(AttentionLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 203,
     "status": "ok",
     "timestamp": 1654159192644,
     "user": {
      "displayName": "NARDJES AMIEUR",
      "userId": "13751943947351102071"
     },
     "user_tz": -120
    },
    "id": "B0DPRBWtj2pY"
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embedding_dim, max_sequence_length, gru_dim,dropout = 0.2):\n",
    "  '''\n",
    "    Create a Bidirectional GRU used for binary text classification task\n",
    "    Arguments : \n",
    "      vocab_size: vocabulary size \n",
    "      embedding_dim: embedding dimensions for the document vector representation \n",
    "      max_sequence_length: input sequence length for each document \n",
    "      gru_dim: number of neurones in the GRU layers\n",
    "      dropout : dropout rate used for Dropout regularization layers to minimize overfitting\n",
    "\n",
    "    Returns : \n",
    "    A model with the folowing layers:  \n",
    "            - Input layer to instantiate a tensor using the input preprocessed data\n",
    "            - Embedding layer to represent each document with a context vector\n",
    "            - Biderectional GRU (takes into consideration bidrectional context)\n",
    "            - A custom attention layer that takes **all** of the output of the GRU ( not the last hidden state only)\n",
    "            - Dense layer \n",
    "            - Dropout layer to minimize overfitting\n",
    "            - A Dense layer \n",
    "            - Dropout \n",
    "            - An output layer to get the label of the class \n",
    "  '''\n",
    "  model = Sequential([\n",
    "      layers.Input(shape=(max_sequence_length,)),\n",
    "      layers.Embedding(vocab_size, embedding_dim, trainable=True),\n",
    "      layers.Bidirectional(layers.GRU(gru_dim, return_sequences=True)),\n",
    "      AttentionLayer(max_sequence_length),\n",
    "      layers.Dense(gru_dim*2, activation='relu'),\n",
    "      layers.Dropout( rate = dropout ),\n",
    "      layers.Dense(gru_dim, activation='relu'),\n",
    "      layers.Dropout( rate = dropout ),\n",
    "      layers.Dense(1, activation='sigmoid')              \n",
    "  ])\n",
    "  model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5807,
     "status": "ok",
     "timestamp": 1654159201017,
     "user": {
      "displayName": "NARDJES AMIEUR",
      "userId": "13751943947351102071"
     },
     "user_tz": -120
    },
    "id": "-PYlxK2wjDdI",
    "outputId": "1c309979-07e4-4c57-c036-ad963ee02dd7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 17:38:02.432461: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-02 17:38:03.060303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10417 MB memory:  -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1000, 128)         5055872   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 1000, 256)        198144    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " attention_layer (AttentionL  (None, 256)              1256      \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,354,089\n",
      "Trainable params: 5,354,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 128 # dimension for dense embeddings for each token\n",
    "GRU_DIM = 128 # number of neurones in the GRU layers\n",
    "VOCAB_SIZE = len(t.word_index)\n",
    "\n",
    "# Create the model\n",
    "model = create_model(vocab_size = VOCAB_SIZE, embedding_dim = EMBEDDING_DIM, max_sequence_length = MAX_SEQUENCE_LENGTH, gru_dim = GRU_DIM)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 849942,
     "status": "ok",
     "timestamp": 1654160050950,
     "user": {
      "displayName": "NARDJES AMIEUR",
      "userId": "13751943947351102071"
     },
     "user_tz": -120
    },
    "id": "E1a2o_x8mQHg",
    "outputId": "7b88e677-3c08-4485-dce4-cb51536590db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 17:38:15.252620: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 28s 67ms/step - loss: 0.3905 - accuracy: 0.7906 - val_loss: 0.1209 - val_accuracy: 0.9608\n",
      "Epoch 2/5\n",
      "360/360 [==============================] - 23s 65ms/step - loss: 0.0962 - accuracy: 0.9680 - val_loss: 0.0994 - val_accuracy: 0.9615\n",
      "Epoch 3/5\n",
      "360/360 [==============================] - 24s 65ms/step - loss: 0.0400 - accuracy: 0.9889 - val_loss: 0.0809 - val_accuracy: 0.9694\n",
      "Epoch 4/5\n",
      "360/360 [==============================] - 23s 65ms/step - loss: 0.0184 - accuracy: 0.9951 - val_loss: 0.0900 - val_accuracy: 0.9740\n",
      "Epoch 5/5\n",
      "360/360 [==============================] - 23s 65ms/step - loss: 0.0075 - accuracy: 0.9982 - val_loss: 0.1094 - val_accuracy: 0.9764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f86605575c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model training \n",
    "batch_size = 32\n",
    "model.fit(X_train, y_train, epochs= 5, batch_size=batch_size, \n",
    "          shuffle=True, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17825,
     "status": "ok",
     "timestamp": 1654160191580,
     "user": {
      "displayName": "NARDJES AMIEUR",
      "userId": "13751943947351102071"
     },
     "user_tz": -120
    },
    "id": "cnelwp1xmXaK",
    "outputId": "475ea755-3d30-4c80-c725-e259a61679e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 4s 27ms/step - loss: 0.0911 - accuracy: 0.9806\n",
      "Accuracy: 98.06%\n"
     ]
    }
   ],
   "source": [
    "# model evaluation \n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21642,
     "status": "ok",
     "timestamp": 1654160296815,
     "user": {
      "displayName": "NARDJES AMIEUR",
      "userId": "13751943947351102071"
     },
     "user_tz": -120
    },
    "id": "9_UW2uhEm58b",
    "outputId": "8812b0db-8948-4890-f380-808b348c628a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 3s 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 1, 1, 0, 1, 1, 0, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get model predictions \n",
    "prediction_probs = model.predict(X_test, verbose=1).ravel()\n",
    "predictions = [1 if prob > 0.5 else 0 for prob in prediction_probs]\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1613,
     "status": "ok",
     "timestamp": 1654160384439,
     "user": {
      "displayName": "NARDJES AMIEUR",
      "userId": "13751943947351102071"
     },
     "user_tz": -120
    },
    "id": "-d1T63Ehi9EE",
    "outputId": "af9fb763-2ade-4ca8-e7cb-693f54f630cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 27ms/step - loss: 1.2938 - accuracy: 0.8191\n",
      "Accuracy: 81.91%\n"
     ]
    }
   ],
   "source": [
    "# save the test results \n",
    "save = pd.read_csv('data/sample_scarcity.csv')\n",
    "\n",
    "norm_ = save['ad']\n",
    "test_ = t.texts_to_sequences(norm_)\n",
    "padded_ = tf.keras.preprocessing.sequence.pad_sequences(test_, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "scores = model.evaluate(padded_, save['scarcity'], verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 1760,
     "status": "ok",
     "timestamp": 1654160394728,
     "user": {
      "displayName": "NARDJES AMIEUR",
      "userId": "13751943947351102071"
     },
     "user_tz": -120
    },
    "id": "xAxtChjhVmsK",
    "outputId": "56b0a9fa-0177-4ec7-b176-815cde218335"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 24ms/step\n",
      "confusion matrix:\n",
      "[[93  8]\n",
      " [28 70]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_ = t.texts_to_sequences(norm_)\n",
    "padded_val = tf.keras.preprocessing.sequence.pad_sequences(val_, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "pred_ =  model.predict(padded_val)\n",
    "save['predicted'] = [1 if prob > 0.5 else 0 for prob in pred_]\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(save['scarcity'], save['predicted']))\n",
    "print()\n",
    "\n",
    "skplt.metrics.plot_confusion_matrix(save['scarcity'], save['predicted'], normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1654160402295,
     "user": {
      "displayName": "NARDJES AMIEUR",
      "userId": "13751943947351102071"
     },
     "user_tz": -120
    },
    "id": "041iI-9aWKSi",
    "outputId": "a3ba67d9-7a23-4dbd-c36e-7bae7e608e6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " no-scarcity       0.77      0.92      0.84       101\n",
      "    scarcity       0.90      0.71      0.80        98\n",
      "\n",
      "    accuracy                           0.82       199\n",
      "   macro avg       0.83      0.82      0.82       199\n",
      "weighted avg       0.83      0.82      0.82       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(save['scarcity'], save['predicted'], target_names=[ 'no-scarcity', 'scarcity']))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bi-GRU_attention_scarcity.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
